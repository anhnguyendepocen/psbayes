\documentclass{slides}
\usepackage{xspace}
\usepackage{nth}
\usepackage{fontawesome}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    urlcolor = blue
}

\title[Bayesian Inference]{Inference in a Poisson model}
\author[Andrews]{Mark Andrews \\ $\phantom{foo}$ \\ Psychology, Nottingham Trent University \\ $\phantom{foo}$ \\ \faEnvelopeO \  \texttt{mark.andrews@ntu.ac.uk} \\ $\phantom{foo}$ \\ \faTwitter \href{https://twitter.com/xmjandrews}{@xmjandrews}, \faTwitter \href{https://twitter.com/priorexposure}{@priorexposure}\\ $\phantom{foo}$ \\ \faGithub \ \url{https://github.com/lawsofthought/psbayes}}

\date{December, 2018}

\begin{document}
{
	\begin{frame}
		\titlepage
	\end{frame}
}


	\begin{frame}
		\frametitle{Inference in Poisson models}
		\begin{itemize}
			\item The Poisson distribution can be used to model the rare occurrences in fixed intervals.
			\item If $x$ is a Poisson random variable with rate $\lambda$, then 
				\[\Prob{x = k \given \lambda} = \frac{\lambda^k e^{-\lambda}}{k!}.\]
		\end{itemize}
		\input{tikZfigs/poisson.distribution.tex}
	\end{frame}
	
	\begin{frame}
		\frametitle{Inference in Poisson models}
		\begin{itemize}
			\item Let's say that the number of emails I get per hour, in $n=10$ hours, is 
				\[D = 6, 8, 12,  7, 11, 14, 10, 12, 11, 16.\]
			\item We can assume that these frequencies are generated according to a Poisson process with rate $\lambda$ per hour.
			\item Given this assumption and this observed data, what is the probable value of $\lambda$?
			\item In other words, what is 
				\[
					\Prob{\lambda \given D}?
				\]
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Likelihood of a Poisson model}
		\begin{itemize}
			\item Given a known value for $\lambda$, the probability of $x_1, x_2 \cdots x_n$ is 
				\begin{align*}
					\Prob{x_1, x_2 \cdots x_n \given \lambda} &= \prod_{i=1}^n \Prob{x_i \given \lambda},\\
					&= \prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!},\\
					&\propto e^{-n\lambda}\prod_{i=1}^n \lambda^{x_i} = e^{-n\lambda} \lambda^{\sum_{i=1}^{n}x_i}.\\
					\intertext{When $D = x_1, x_2 \cdots x_n = 6, 8 \cdots 16$, the likelihood of $\lambda$ is}
					\Prob{D \given \lambda} &= e^{-n\lambda} \lambda^{S},
				\end{align*}
				where $S = \sum_{i=1}^{n} x_i = 107$.
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Likelihood of a Poisson model}
		\input{tikZfigs/poisson.likelihood.tex}

		The likelihood of $\lambda$ given the sufficient statistics $S=107$ and $n=10$.
	\end{frame}

	\begin{frame}
		\frametitle{Conjugate prior for the Poisson model}
		\begin{itemize}
			\item The Gamma distribution with shape $\kappa$ and scale $\theta$ is a conjugate prior for the Poisson model:
				\[
					\textrm{Gamma}(\lambda \given \kappa, \theta) = \frac{\lambda^{\kappa-1} e^{-\lambda/\theta}}{\theta^\kappa \Gamma(\kappa)}.
				\]
		\end{itemize}

		\hspace*{-5mm}
		\input{tikZfigs/gamma.distributions.tex}

	\end{frame}

	\begin{frame}
		\frametitle{Posterior distribution}
		\begin{itemize}
	
			\item With the Poisson likelihood, the Gamma prior leads to 
				\begin{align*}
					\Prob{\lambda\given D, \kappa,\theta} &\propto  e^{-n\lambda} \lambda^{S} \times \frac{\lambda^{\kappa-1} e^{-\lambda/\theta}}{\theta^\kappa \Gamma(\kappa)},\\
					&\propto e^{-\lambda\left(n+\tfrac{1}{\theta}\right)} \lambda^{S + \kappa - 1}.
				\end{align*}
				Given that 
				\begin{align*}
					\int e^{-\lambda\left(n+\tfrac{1}{\theta}\right)} \lambda^{S + \kappa - 1}\ d\lambda = \left(n + \tfrac{1}{\theta}\right)^{-(S+\kappa)}\Gamma(S+\kappa),
				\end{align*}
				we have
				\[
					\Prob{\lambda\given D, \kappa,\theta} = \textrm{Gamma}(\lambda \given S+\kappa, (n + \tfrac{1}{\theta})^{-1}).
				\]
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Posterior distribution}
		\begin{itemize}
	
			\item With prior hyper-parameters of $\kappa=1.0$, $\theta=100.0$, and sufficient statistics of $S=107$ and $n=10$, we have Gamma distribution with shape $108$ and scale $\approx 0.1$
		\end{itemize}

		\hspace*{-5mm}
		\input{tikZfigs/gamma.posterior}
	\end{frame}

	\begin{frame}
		\frametitle{Summarizing the posterior}
		\begin{itemize}
		\item The mean, variance and modes of any Gamma distribution with shape $\kappa$ and scale $\theta$ are as follows:
			\begin{align*}
				\langle \lambda \rangle &= \kappa\theta,\\
				\mathrm{V}(\lambda) &= \kappa\theta^2,\\
				\textrm{mode}(\lambda) &= (\kappa-1)\theta.
			\end{align*}
		\item Thus in our case, we have
			\begin{align*}
				\langle \lambda \rangle &= 108.0,\\
				\mathrm{V}(\lambda) &= 1.08, \quad\textrm{sd}(\theta) = 1.04,\\
				\textrm{mode}(\lambda) &= 10.69.
			\end{align*}
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{The 0.95 \hpd interval}

		\input{tikZfigs/gamma.posterior.hpd}

		The 0.95 \hpd interval is from $8.78$ to $12.86$.
		
	\end{frame}

	\begin{frame}

		\frametitle{Posterior predictive distribution}

		\begin{itemize}
			\item Given that the number of emails every hour were
				\[D = 6, 8, 12,  7, 11, 14, 10, 12, 11, 16,\]
				what do we predict will be number of emails in the next hour?
			\item This is given by the posterior predictive distribution:
				\begin{align*}
					\Prob{x_{\textrm{next}}= k \given D, \kappa, \theta} &= \int \Prob{x_{\textrm{next}} = k \given \lambda}\Prob{\lambda\given D, \kappa, \theta} d\lambda,\\
					&= \int \frac{e^{-k}\lambda^k}{k!} \times \frac{(n+\tfrac{1}{\theta})^{S+\kappa}}{\Gamma(S+\kappa)} e^{-\lambda(n+\tfrac{1}{\theta})} \lambda^{S+\kappa-1},\\
					&= \frac{\Gamma(S+\kappa+k)}{\Gamma(S+\kappa)\Gamma(k+1)} q^{k} (1-q)^{S+\kappa} ,\\
					&= \textrm{NegativeBinomial}(k\given S+\kappa, q),
				\end{align*}
				with $q = \left(n+\tfrac{1}{\theta} + 1\right)^{-1}$. 

		\end{itemize}

	\end{frame}

	\begin{frame}
		\frametitle{Posterior predictive distribution}
		\begin{itemize}
			\item The negative binomial distribution gives the number of ``successes'' until a predefined number $r$ of ``failures'' have occurred, with the probability of a success being $q$.
			\item The mean and variance of the negative binomial are \[\langle k \rangle = \frac{qr}{1-q}, \quad  V(k) = \frac{qr}{(1-q)^2}. \]
			\item In our case, $r=S+\kappa=108$ and $q=\left(n+\tfrac{1}{\theta} + 1\right)^{-1} = \left(10+\tfrac{1}{100} + 1\right)^{-1} = 0.091$.
			\item The mean and variance of the negative binomial are 
				\[\langle k \rangle = \frac{S+\kappa}{n+\tfrac{1}{\theta}} = 10.79, \quad  V(k) = \frac{S+\kappa}{n+\tfrac{1}{\theta}} (n + \frac{1}{\theta} + 1) = 11.87. \]
		\end{itemize}
	\end{frame}


	\begin{frame}
		\frametitle{Posterior predictive distribution}

		\hspace*{-5mm}
		\input{tikZfigs/poisson.posterior.predictive}

		The posterior predictive distribution $\Prob{x_{\textrm{next}}=k\given D, \kappa, \theta}$ is a negative binomial distribution with $r=108$, $q=0.091$.
	\end{frame}


\end{document}
